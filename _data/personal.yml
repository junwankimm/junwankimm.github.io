name: Junwan KIM

label: AI Researcher

picture: /img/me.JPG

email: jw1510@yonsei.ac.kr

phone: +82-10-9353-7393

department: Artificial Intelligence

institution: Student @ Yonsei Univiersity

mailing_address: >
  50, Yonsei-ro, Seodaemun-gu, Seoul, Republic of Korea

profiles:
  - 
    network: Google Scholar
    username: Junwan KIM
    url: http://scholar.google.com/
  -
    network: LinkedIn
    username: Junwan KIM
    url: https://linkedin.com/in/junwan-kim-39b3aa287
  -
    network: Notion
    username: Junwan KIM
    url: https://kimjuwan.notion.site
  -
    network: CV
    username: Junwan KIM
    url: https://drive.google.com/file/d/1sbE0I6Gqhce8jyLFl2x5fIyZLOrYP4gT/view?usp=drive_link

intro:
  title: Junwan Kim
  body: >
    _**Human can think beyond a single modality**_, such as seeing a scene (visual representations) and 
    conjuring up poems (language representations) about it, or imagining the sounds (audio representations) 
    associated with that scene, or so on.

    <br/><br/>In this context, I believe that imitating humans through integration of such modalities is  
    ultimately the direction AI should pursue. Therefore, _**my primary research lie in the field of artificial intelligence,  
    particularly but not limited to computer vision and multimodal. Specifically, Vision-Language Models (VLMs), Multimodal-Large-Language Models (MLLMs)**_
