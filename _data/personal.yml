name: Jun-Wan KIM

label: AI Developer

picture: /img/cat.jpg

email: jw1510@yonsei.ac.kr

phone: +82-10-9353-7393

department: Artificial Intelligence, Computer Vision, Multimodal

institution: Yonsei University

mailing_address: >
  50, Yonsei-ro, Seodaemun-gu, Seoul, Republic of Korea

profiles:
  - 
    network: Google Scholar
    username: Junwan KIM
    url: http://scholar.google.com/
  -
    network: LinkedIn
    username: Junwan KIM
    url: https://www.linkedin.com/
  -
    network: Twitter
    username: Junwan KIM
    url: https://twitter.com/

intro:
  title: Bio
  body: >
    Human can think beyond a single modality, such as seeing a scene (visual representations) and 
    conjuring up poems (language representations) about it, or imagining the sounds (audio representations) 
    associated with that scene, or so on.  

    In this context, I believe that imitating humans through integration of such modalities is  
    ultimately the direction AI should pursue. Therefore, my primary research lie in the field of artificial intelligence,  
    particularly but not limited to multimodal. Specifically, Vision-Language Models (VLMs), Multimodal-Large-Language Models (MLLMs)
